{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = webdriver.FirefoxOptions()\n",
    "option.add_argument('headless') # Reduce chance of getting identified"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping world indices prices from Yahoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Firefox()\n",
    "url = 'https://finance.yahoo.com/quote/%5EGSPC/history?p=%5EGSPC'\n",
    "browser.get(url)\n",
    "time.sleep(5) # Let it load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = browser.find_element(By.TAG_NAME,'tbody')\n",
    "\n",
    "elements = tables.find_elements(By.TAG_NAME, 'span') # span contains the data in each cell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List 1: ['Jun 30, 2023', 'Jun 29, 2023', 'Jun 28, 2023', 'Jun 27, 2023', 'Jun 26, 2023']\n",
      "List 2: ['4,422.44', '4,374.94', '4,367.48', '4,337.36', '4,344.84']\n",
      "List 3: ['4,458.48', '4,398.39', '4,390.35', '4,384.42', '4,362.06']\n",
      "List 4: ['4,422.44', '4,371.97', '4,360.22', '4,335.00', '4,328.08']\n",
      "List 5: ['4,450.38', '4,396.44', '4,376.86', '4,378.41', '4,328.82']\n",
      "List 6: ['4,450.38', '4,396.44', '4,376.86', '4,378.41', '4,328.82']\n",
      "List 7: ['3,923,450,000', '3,696,660,000', '3,739,330,000', '3,573,500,000', '3,415,030,000']\n"
     ]
    }
   ],
   "source": [
    "list1 = []\n",
    "list2 = []\n",
    "list3 = []\n",
    "list4 = []\n",
    "list5 = []\n",
    "list6 = []\n",
    "list7 = []\n",
    "\n",
    "lists = [list1, list2, list3, list4, list5, list6, list7]  # List of all target lists\n",
    "index = 0  # Starting point\n",
    "\n",
    "for i, element in enumerate(elements):\n",
    "    target_list = lists[index]  # When we are iterating the first element in our output, we will simultaneously select list1 because lists[0] is list1\n",
    "    target_list.append(element.text)  # On the same iteration and with list1 selected, append the iterated element to list1\n",
    "\n",
    "    # Update the index to cycle through the target lists\n",
    "    index = (index + 1) % len(lists)\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# Printing out the list.\n",
    "print(\"List 1:\", list1[:5])\n",
    "print(\"List 2:\", list2[:5])\n",
    "print(\"List 3:\", list3[:5])\n",
    "print(\"List 4:\", list4[:5])\n",
    "print(\"List 5:\", list5[:5])\n",
    "print(\"List 6:\", list6[:5])\n",
    "print(\"List 7:\", list7[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jun 30, 2023</td>\n",
       "      <td>4,422.44</td>\n",
       "      <td>4,458.48</td>\n",
       "      <td>4,422.44</td>\n",
       "      <td>4,450.38</td>\n",
       "      <td>4,450.38</td>\n",
       "      <td>3,923,450,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jun 29, 2023</td>\n",
       "      <td>4,374.94</td>\n",
       "      <td>4,398.39</td>\n",
       "      <td>4,371.97</td>\n",
       "      <td>4,396.44</td>\n",
       "      <td>4,396.44</td>\n",
       "      <td>3,696,660,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jun 28, 2023</td>\n",
       "      <td>4,367.48</td>\n",
       "      <td>4,390.35</td>\n",
       "      <td>4,360.22</td>\n",
       "      <td>4,376.86</td>\n",
       "      <td>4,376.86</td>\n",
       "      <td>3,739,330,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jun 27, 2023</td>\n",
       "      <td>4,337.36</td>\n",
       "      <td>4,384.42</td>\n",
       "      <td>4,335.00</td>\n",
       "      <td>4,378.41</td>\n",
       "      <td>4,378.41</td>\n",
       "      <td>3,573,500,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jun 26, 2023</td>\n",
       "      <td>4,344.84</td>\n",
       "      <td>4,362.06</td>\n",
       "      <td>4,328.08</td>\n",
       "      <td>4,328.82</td>\n",
       "      <td>4,328.82</td>\n",
       "      <td>3,415,030,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Feb 13, 2023</td>\n",
       "      <td>4,096.62</td>\n",
       "      <td>4,138.90</td>\n",
       "      <td>4,092.67</td>\n",
       "      <td>4,137.29</td>\n",
       "      <td>4,137.29</td>\n",
       "      <td>3,448,620,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Feb 10, 2023</td>\n",
       "      <td>4,068.92</td>\n",
       "      <td>4,094.36</td>\n",
       "      <td>4,060.79</td>\n",
       "      <td>4,090.46</td>\n",
       "      <td>4,090.46</td>\n",
       "      <td>3,891,520,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Feb 09, 2023</td>\n",
       "      <td>4,144.25</td>\n",
       "      <td>4,156.23</td>\n",
       "      <td>4,069.67</td>\n",
       "      <td>4,081.50</td>\n",
       "      <td>4,081.50</td>\n",
       "      <td>4,270,200,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Feb 08, 2023</td>\n",
       "      <td>4,153.47</td>\n",
       "      <td>4,156.85</td>\n",
       "      <td>4,111.67</td>\n",
       "      <td>4,117.86</td>\n",
       "      <td>4,117.86</td>\n",
       "      <td>4,029,820,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Feb 07, 2023</td>\n",
       "      <td>4,105.35</td>\n",
       "      <td>4,176.54</td>\n",
       "      <td>4,088.39</td>\n",
       "      <td>4,164.00</td>\n",
       "      <td>4,164.00</td>\n",
       "      <td>4,355,860,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date      open      high       low     close adj_close  \\\n",
       "0   Jun 30, 2023  4,422.44  4,458.48  4,422.44  4,450.38  4,450.38   \n",
       "1   Jun 29, 2023  4,374.94  4,398.39  4,371.97  4,396.44  4,396.44   \n",
       "2   Jun 28, 2023  4,367.48  4,390.35  4,360.22  4,376.86  4,376.86   \n",
       "3   Jun 27, 2023  4,337.36  4,384.42  4,335.00  4,378.41  4,378.41   \n",
       "4   Jun 26, 2023  4,344.84  4,362.06  4,328.08  4,328.82  4,328.82   \n",
       "..           ...       ...       ...       ...       ...       ...   \n",
       "95  Feb 13, 2023  4,096.62  4,138.90  4,092.67  4,137.29  4,137.29   \n",
       "96  Feb 10, 2023  4,068.92  4,094.36  4,060.79  4,090.46  4,090.46   \n",
       "97  Feb 09, 2023  4,144.25  4,156.23  4,069.67  4,081.50  4,081.50   \n",
       "98  Feb 08, 2023  4,153.47  4,156.85  4,111.67  4,117.86  4,117.86   \n",
       "99  Feb 07, 2023  4,105.35  4,176.54  4,088.39  4,164.00  4,164.00   \n",
       "\n",
       "              vol  \n",
       "0   3,923,450,000  \n",
       "1   3,696,660,000  \n",
       "2   3,739,330,000  \n",
       "3   3,573,500,000  \n",
       "4   3,415,030,000  \n",
       "..            ...  \n",
       "95  3,448,620,000  \n",
       "96  3,891,520,000  \n",
       "97  4,270,200,000  \n",
       "98  4,029,820,000  \n",
       "99  4,355,860,000  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'date': list1,\n",
    "    'open': list2,\n",
    "    'high': list3,\n",
    "    'low': list4,\n",
    "    'close': list5,\n",
    "    'adj_close': list6,\n",
    "    'vol': list7\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can see that the static code only extracts 100 lines of data from the site, however if we access the browser and scroll manually the data spans across one year. \n",
    "\n",
    "##### Dynamic website promotes better site loading speed but limits our result if we scrape it statically. We will re-scrape Yahoo by automating a scrolling action using selenium javascript module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data for index: https://finance.yahoo.com/quote/%5EGSPC/history?p=%5EGSPC\n",
      "Data saved to: GSPC.csv\n",
      "Scraped data for index: https://finance.yahoo.com/quote/%5EDJI/history?p=%5EDJI\n",
      "Data saved to: DJI.csv\n",
      "Scraped data for index: https://finance.yahoo.com/quote/%5EIXIC/history?p=%5EIXIC\n",
      "Data saved to: IXIC.csv\n",
      "Scraped data for index: https://finance.yahoo.com/quote/%5ENYA/history?p=%5ENYA\n",
      "Data saved to: NYA.csv\n",
      "Scraped data for index: https://finance.yahoo.com/quote/%5ERUT/history?p=%5ERUT\n",
      "Data saved to: RUT.csv\n",
      "Scraped data for index: https://finance.yahoo.com/quote/%5EFTSE/history?p=%5EFTSE\n",
      "Data saved to: FTSE.csv\n",
      "Scraped data for index: https://finance.yahoo.com/quote/%5EGDAXI/history?p=%5EGDAXI\n",
      "Data saved to: GDAXI.csv\n",
      "Scraped data for index: https://finance.yahoo.com/quote/%5EFCHI/history?p=%5EFCHI\n",
      "Data saved to: FCHI.csv\n",
      "Scraped data for index: https://finance.yahoo.com/quote/%5ESTOXX50E/history?p=%5ESTOXX50E\n",
      "Data saved to: STOXX50E.csv\n",
      "Scraped data for index: https://finance.yahoo.com/quote/%5EN225/history?p=%5EN225\n",
      "Data saved to: N225.csv\n",
      "Scraped data for index: https://finance.yahoo.com/quote/%5EHSI/history?p=%5EHSI\n",
      "Data saved to: HSI.csv\n",
      "Scraped data for index: https://finance.yahoo.com/quote/%5ESTI/history?p=%5ESTI\n",
      "Data saved to: STI.csv\n",
      "Scraped data for index: https://finance.yahoo.com/quote/%5EKLSE/history?p=%5EKLSE\n",
      "Data saved to: KLSE.csv\n"
     ]
    }
   ],
   "source": [
    "browser = webdriver.Firefox()\n",
    "\n",
    "url_list = [\n",
    "    'https://finance.yahoo.com/quote/%5EGSPC/history?p=%5EGSPC',\n",
    "    'https://finance.yahoo.com/quote/%5EDJI/history?p=%5EDJI',\n",
    "    'https://finance.yahoo.com/quote/%5EIXIC/history?p=%5EIXIC',\n",
    "    'https://finance.yahoo.com/quote/%5ENYA/history?p=%5ENYA',\n",
    "    'https://finance.yahoo.com/quote/%5ERUT/history?p=%5ERUT',\n",
    "    'https://finance.yahoo.com/quote/%5EFTSE/history?p=%5EFTSE',\n",
    "    'https://finance.yahoo.com/quote/%5EGDAXI/history?p=%5EGDAXI',\n",
    "    'https://finance.yahoo.com/quote/%5EFCHI/history?p=%5EFCHI',\n",
    "    'https://finance.yahoo.com/quote/%5ESTOXX50E/history?p=%5ESTOXX50E',\n",
    "    'https://finance.yahoo.com/quote/%5EN225/history?p=%5EN225',\n",
    "    'https://finance.yahoo.com/quote/%5EHSI/history?p=%5EHSI',\n",
    "    'https://finance.yahoo.com/quote/%5ESTI/history?p=%5ESTI',\n",
    "    'https://finance.yahoo.com/quote/%5EKLSE/history?p=%5EKLSE'\n",
    "]\n",
    "\n",
    "for url in url_list:\n",
    "    load_time = random.uniform(3, 5)\n",
    "    wait_time = random.uniform(1, 3)\n",
    "    # Mimic a random behavior to prevent alerting anti-scraping software on the site.\n",
    "\n",
    "    filename = url.split('p=%5E')[1] + '.csv' # CSV name for each ticker\n",
    "\n",
    "    browser.get(url)\n",
    "    time.sleep(load_time)\n",
    "\n",
    "    previous_height = browser.execute_script(\"return document.documentElement.scrollHeight;\") # Get initial height\n",
    "\n",
    "    while True:\n",
    "        browser.execute_script(\"window.scrollBy(0, document.documentElement.scrollHeight);\") # Simulate scrolling, scroll to bottom to trigger dynamic loading\n",
    "\n",
    "        time.sleep(wait_time) # wait a moment before each scroll\n",
    "\n",
    "        current_height = browser.execute_script(\"return document.documentElement.scrollHeight;\") # Get height after scroll\n",
    "\n",
    "        if current_height == previous_height: # If height values are different, page is dynamic, continue loop. Otherwise break the loop\n",
    "            break\n",
    "\n",
    "        previous_height = current_height # Update the height to start a new loop\n",
    "\n",
    "    tables = browser.find_element(By.TAG_NAME,'tbody') \n",
    "    elements = tables.find_elements(By.TAG_NAME, 'span')\n",
    "\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    list3 = []\n",
    "    list4 = []\n",
    "    list5 = []\n",
    "    list6 = []\n",
    "    list7 = []\n",
    "\n",
    "    lists = [list1, list2, list3, list4, list5, list6, list7]\n",
    "    index = 0 \n",
    "\n",
    "    for i, element in enumerate(elements):\n",
    "        target_list = lists[index] \n",
    "        target_list.append(element.text) \n",
    "        index = (index + 1) % len(lists)\n",
    "\n",
    "    with open (filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['date', 'open', 'high', 'low', 'close', 'adj_close', 'vol'])\n",
    "        writer.writerows(zip(list1, list2, list3, list4, list5, list6, list7))\n",
    "\n",
    "    print(f\"Scraped data for index: {url}\")\n",
    "    print(f\"Data saved to: {filename}\")\n",
    "\n",
    "browser.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Shopee for 'Trail Running Shoes' keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shopee has a verification process when it detects a new browser instance, this step has to be done manually.\n",
    "\n",
    "##### After manual verification we will store the cookies and load it to bypass the verification step for subsequent scrape.\n",
    "\n",
    "##### This step only has to be done ONCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Firefox()\n",
    "\n",
    "username = '' # insert your own username\n",
    "password = '' # insert your own password\n",
    "\n",
    "browser.get('https://shopee.com.my/buyer/login?next=https%3A%2F%2Fshopee.com.my%2F') # Login page\n",
    "\n",
    "lang = WebDriverWait(browser, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, \"//button[text()='English']\"))\n",
    ")\n",
    "lang.click() # Click language popup window\n",
    "\n",
    "browser.find_element(By.CLASS_NAME,'pDzPRp').send_keys(username)\n",
    "\n",
    "time.sleep(1)\n",
    "browser.find_element(By.CSS_SELECTOR, '.vkgBkQ > div:nth-child(1) > input:nth-child(1)').send_keys(password)\n",
    "\n",
    "time.sleep(1)\n",
    "browser.find_element(By.CSS_SELECTOR, '.wyhvVD').click()\n",
    "\n",
    "# Manually verify from here on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the below code cell once manually verified and ended at the landing page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(browser.get_cookies(), open(\"cookies.pkl\", \"wb\")) # Store the cookies\n",
    "\n",
    "browser.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staring a new scraping instance for 'Trail Running Shoes'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For subsequent scraping we only have to run the code below since we have cookies stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 0 scraped and appended\n",
      "Page 1 scraped and appended\n",
      "Page 2 scraped and appended\n",
      "Page 3 scraped and appended\n",
      "Page 4 scraped and appended\n",
      "Page 5 scraped and appended\n",
      "Page 6 scraped and appended\n",
      "Page 7 scraped and appended\n",
      "Page 8 scraped and appended\n",
      "Page 9 scraped and appended\n",
      "Data stored in CSV: shopee_trail_shoes.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3v/nvf5sb85235fjz6fgzjnl1gr0000gn/T/ipykernel_38844/2441699372.py:72: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['unit_sold'] = df['unit_sold'].str.split().str[0].str.replace('k', '000').str.replace('.', '').astype(float) # Remove any formatting and extract numbers only\n"
     ]
    }
   ],
   "source": [
    "url = 'https://shopee.com.my/search?keyword=trail%20running%20shoes&page=' # replace with own link to interested search keyword \n",
    "browser = webdriver.Firefox()\n",
    "browser.get(url)\n",
    "cookies = pickle.load(open('cookies.pkl', 'rb')) # This bypasses future verification\n",
    "\n",
    "for cookie in cookies:\n",
    "    browser.add_cookie(cookie)\n",
    "\n",
    "# Elements I'm interested to scrape\n",
    "description_list = []\n",
    "price_list = []\n",
    "unit_sold_list = []\n",
    "location_list = []\n",
    "links_list = []\n",
    "\n",
    "for x in range(0, 10): # I want to scrape 10 pages\n",
    "    load_time = random.uniform(3, 5)\n",
    "    wait_time = random.uniform(1, 3)\n",
    "\n",
    "    page_num = url + str(x)\n",
    "    browser.get(page_num)\n",
    "    time.sleep(load_time)\n",
    "\n",
    "    previous_height = browser.execute_script(\"return document.documentElement.scrollHeight;\")\n",
    "\n",
    "    while True:\n",
    "        browser.execute_script(\"window.scrollBy(0, 1200);\")\n",
    "\n",
    "        time.sleep(wait_time)\n",
    "\n",
    "        current_height = browser.execute_script(\"return document.documentElement.scrollHeight;\")\n",
    "\n",
    "        if current_height == previous_height:\n",
    "            break\n",
    "\n",
    "        previous_height = current_height\n",
    "\n",
    "    wait = WebDriverWait(browser, 10) # Wait a max of 10 seconds\n",
    "    wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'shopee-search-item-result__items'))) # Ensures the entire page has loaded with the elements I want\n",
    "    full_list = browser.find_elements(By.CLASS_NAME, 'shopee-search-item-result__items')\n",
    "\n",
    "    for i in full_list:\n",
    "        description = i.find_elements(By.CLASS_NAME, 'Cve6sh')\n",
    "        description_text = [desc.text for desc in description]\n",
    "        description_list.extend(description_text)\n",
    "        \n",
    "        price = i.find_elements(By.CLASS_NAME, 'rVLWG6')\n",
    "        price_text = [pric.text.split()[0] for pric in price]\n",
    "        price_list.extend(price_text)\n",
    "\n",
    "        unit_sold = i.find_elements(By.CLASS_NAME, 'r6HknA')\n",
    "        unit_sold_text = [unit.text for unit in unit_sold]\n",
    "        unit_sold_list.extend(unit_sold_text)\n",
    "        \n",
    "        location = i.find_elements(By.CLASS_NAME, \"zGGwiV\")\n",
    "        location_text = [loc.text for loc in location]\n",
    "        location_list.extend(location_text)\n",
    "\n",
    "        links = i.find_elements(By.CSS_SELECTOR, \".shopee-search-item-result__item [href]\")\n",
    "        links_text = [link.get_attribute('href') for link in links]\n",
    "        links_list.extend(links_text)\n",
    "\n",
    "        print(f'Page {x} scraped and appended')\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "df = pd.DataFrame({'description':description_list, 'price': price_list, 'unit_sold':unit_sold_list, 'location':location_list, 'url':links_list})\n",
    "\n",
    "df['price'] = df['price'].str.replace(',', '').str.split('RM').str[1].astype(float) # Remove any formatting and extract numbers only\n",
    "df['price'] = df['price'].fillna('')\n",
    "\n",
    "df['unit_sold'] = df['unit_sold'].str.split().str[0].str.replace('k', '000').str.replace('.', '').astype(float) # Remove any formatting and extract numbers only\n",
    "df['unit_sold'] = df['unit_sold'].fillna('')\n",
    "\n",
    "df.to_csv('shopee_trail_shoes.csv', index=True)\n",
    "\n",
    "print('Data stored in CSV: shopee_trail_shoes.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
